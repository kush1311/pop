name: Daily PPO Update
 
on:
  schedule:
    - cron: "30 10 * * 1-5"  # â° Runs at 10:30 AM UTC = 4:00 PM IST (Monâ€“Fri)
  workflow_dispatch:
 
jobs:
  retrain:
    runs-on: ubuntu-latest
 
    env:
      NEWSDATA_API_KEY: ${{ secrets.NEWSDATA_API_KEY }}
      FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
      GNEWS_API_KEY: ${{ secrets.GNEWS_API_KEY }}
 
    steps:
      - name: ðŸ“… Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history and tags
          lfs: true       # Enable Git LFS to fetch large model files

      - name: ðŸ“„ List repository contents to debug
        run: |
          echo "=== Listing base directory ==="
          ls -la
          echo "=== Checking saved_models_with_xgb directory ==="
          ls -la saved_models_with_xgb || echo "Directory not found"
          echo "=== Checking saved_envs directory ==="
          ls -la saved_envs || echo "Directory not found"
 
      - name: ðŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
 
      - name: ðŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ðŸ”§ Create required directories
        run: |
          mkdir -p saved_models_with_xgb
          mkdir -p saved_envs
          mkdir -p daily_reports
          mkdir -p daily_predictions
          echo "Created required directories for model storage"
          
      - name: ðŸ“¦ Copy model files if needed
        run: |
          if [ -d "$GITHUB_WORKSPACE/pop/saved_models_with_xgb" ]; then
            echo "Copying models from pop/saved_models_with_xgb"
            cp -r $GITHUB_WORKSPACE/pop/saved_models_with_xgb/* saved_models_with_xgb/
          fi
          
          if [ -d "$GITHUB_WORKSPACE/saved_models_with_xgb" ]; then
            echo "Listing found model files:"
            ls -la saved_models_with_xgb/
          else
            echo "No model directory found after creation"
          fi
 
      - name: ðŸš€ Run daily PPO update
        run: |
          python run_daily_update.py
 
      - name: ðŸ”® Generate predictions
        run: |
          # First check if we have any models
          echo "Model files available:"
          ls -la saved_models_with_xgb/ || echo "No models directory found"
          
          # Run prediction script
          python generate_predictions.py
          echo "--- Generated prediction files ---"
          ls -lh daily_predictions/
 
      - name: ðŸ“‚ List output files
        run: |
          echo "--- Files in workspace ---"
          ls -lh
 
      - name: ðŸ“¤ Upload OHLCV as artifact
        uses: actions/upload-artifact@v4
        with:
          name: live-ohlcv-data
          path: live_nifty50_features.csv
 
      - name: ðŸ“¤ Upload Daily Summary
        uses: actions/upload-artifact@v4
        with:
          name: daily-summary
          path: daily_nifty50_summary.csv
 
      - name: ðŸ“¤ Upload Daily Report (Optional Backup)
        uses: actions/upload-artifact@v4
        with:
          name: retraining-reports
          path: daily_reports/*.csv
      
      - name: ðŸ“¤ Upload Excel file (if updated)
        uses: actions/upload-artifact@v4
        with:
          name: nifty50-processed-features
          path: nifty50_processed_features.xlsx
          if-no-files-found: ignore
          
      - name: ðŸ“¤ Upload Predictions (Dated)
        run: echo "DATE_TAG=$(date +'%Y%m%d')" >> $GITHUB_ENV
      
      - name: Upload with date
        uses: actions/upload-artifact@v4
        with:
          name: daily-predictions-${{ env.DATE_TAG }}
          path: daily_predictions/*.csv
          if-no-files-found: warn


